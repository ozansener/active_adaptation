\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{wrapfig}
\usepackage{graphicx} % more modern
\usepackage{caption}
\usepackage{subcaption}

% AMS stuff
\usepackage{amsmath, amssymb}
\usepackage{dsfont}

%\usepackage{ntheorem}
\usepackage[amsthm,thmmarks]{ntheorem}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{proposition}{Proposition}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{xspace}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}


\title{A Geometric Approach to Active Learning for Convolutional Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract} 
Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;
    training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a
    large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a
    very large collection (\ie active learning).

In this paper, we first empirically study the effectiveness of the many active learning heuristics.
    Our empirical study suggests that many of these heuristics are not effective even in oracle case when applied to CNNs. Inspired by these results, 
    we re-define the problem of active learning as \emph{core-set} selection. In other words, we define the active learning problem as choosing set of
    points such that a model learned over the selected subset is competitive for the remaining data points. We further present a bound over the
    difference between the average loss over any subset of the dataset and the remainder. And, we directly minimize this bound as an active learning algorithm. 
    Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin. 
\end{abstract} 

\section{introduction}
Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, like
image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback;
they need a very large amount of labeled data to be able to learn their millions of parameters. More importantly, it is almost always better to have
more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more
data. Although this is the behavior you want from an algorithmic perspective (higher representative power is typically better), labeling a dataset is
a time consuming and an expensive task. These practical considerations raise a critical question: \emph{``what is the optimal way to choose data
points to label such that the highest accuracy can be obtained given a fixed labeling budget.''} Active learning is one of the common paradigms to
address this question.

The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the
accuracy. Although it is not possible to obtain a universally good active learning strategy \cite{dasgupta2004analysis}, there exist many
heuristics~\cite{settles2010active} which have been proven to be effective in practice. Active learning is typically an iterative process in which a
model is learned at each iteration and a set of points is chosen from a pool of unlabelled points using these aforementioned heuristics. We experiment
with many of these heuristics in this paper and find them not effective when applied to CNNs. Following an extensive empirical study, we argue that
the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In classical settings, the active learning algorithms
typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point likely to have no statistically
significant impact on the accuracy due to the local optimization methods, and ii) at each iteration requires a full training until convergence which
makes the algorithm computationally intractable. Hence, we choose a large batch at each iteration and it results in correlated samples even for
moderetely small subset sizes.


\bibliography{active_learning}
\bibliographystyle{iclr2018_conference}

\end{document}
