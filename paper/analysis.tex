In this section, we analyze our algorithm in terms of generalization error.  We are typically interested in the error in unseen images $E_{\mathbf{x},y \sim p_\mathcal{Z}}[l(\mathbf{x},y,A_{\mathbf{s}})]$ in terms of the empirical loss over the labelled images $\frac{1}{m}\sum_j l(\mathbf{x}_{s(i)},y_{s(i)},A_{\mathbf{s}})$. However, this analysis requires joint treatment of the generalization error and the effect of query selection. For simplicity, we divide this analysis into two parts. First, we analyze the relationship between expected loss in unseen images (generalization error) and the empirical loss over the entire dataset ($\frac{1}{n}\sum_i l(\mathbf{x}_i,y_i,A_\mathbf{s})$). Secondly, we analyze the relationship between the loss over entire dataset and empirical loss over the qeried samples. Our analysis is largely based on the gometry of the space and the geometric properties of the loss function. We rely on Lipschitz continuity of loss function and regression function. 

We study the generalization error by assuming a Lipschitz continuous loss function for a CNN. By extending the robustness results from \cite{robust}, we state the following theorem and defer its proof to the supplementary material.
\begin{theorem}
Given $n$ i.i.d. samples drawn from $p_\mathcal{Z}$ as $\{\mathbf{x}_i,y_i\}_{i\in[n]}$. If loss function $l(\cdot,y,\mathbf{w})$ is $\lambda^l$-Lipschitz continuous for all $y, \mathbf{w}$, bounded by $L$ and $\mathcal{X}x\mathcal{Y}$ has a covering number $N_{\epsilon}(\mathcal{X},|\cdot|_2)=K$; with probability at least $(1-\gamma)$,
\[
\begin{aligned}
&\left|E_{\mathbf{x},y \sim p_\mathcal{Z}}[l(\mathbf{x},y, A_\mathbf{s})] - \frac{1}{n}\sum_i l(\mathbf{x}_i,y_i,A_\mathbf{s})\right| \\
 &\hspace{3cm}\leq  \lambda^l \epsilon + L \sqrt{\frac{2K\ln 2 + 2\ln (1/\gamma)}{n}}\\
\end{aligned}
\]
\label{mainthm}
\end{theorem}

First of all, this theorem is applicable to any machine learning algorithm with Lipschitz loss function and we further prove the Lipschitz-continuity of CNNs. It can clearly be seen that the empirical loss converges to expected loss with large number of data points $n$ since $\lambda^l\epsilon$ term can be made arbitrarily small as long as $\mathcal{X}$ is a compact space. In order to complete the study about the generalization performance of CNNs, we prove the Lipschitz-continuity of the loss function of a CNN with the following lemma where max-pool and restricted liner units are the non-linearities and the loss is defined as $l_2$ distance between the desired probabilities and the soft-max outputs.

\begin{lemma}
A convolutional neural network with $n_c$ convolutional (with max-pool and ReLU) and $n_{fc}$ fully connected layers defined over C class with loss function defined as 2-norm between softmax and class probability is $\left(\frac{\sqrt{C-1}}{C} \alpha^{n_c+n_{fc}}\right)$-Lipschitz.
\end{lemma}

Here, $\alpha$ is the maximum sum of  input weights per neuron (see supplementary materials for detailed definition). Although, it is in general unbounded, it can be made arbitrarily small without changing loss function behavior (ie. keeping the label of any data point $\mathbf{s}$ unchanged) since dividing all weights with a scalar will not switch labels. Hence, for any CNN, there is equivalent CNN (in terms of classification function) with $\alpha \leq \varrho$ for any $\varrho > 0$. We can conclude that CNNs enjoy a $0$ generalization error in limiting case thanks to the Lipschitz property.

In order to complete the analysis, we need to study the behavior of the loss over the dataset in terms of the empirical loss over the selected(queried) samples. Here, we make a no training error assumption; in other words, we assume that the training error for labelled images is $0$ at the end of the learning. This is clearly a restrictive assumption; however, it is very feasible since CNNs are very representative because of their large parameter space. Moreover, this can also be easily enforced by simply converting average loss into maximal loss via \cite{maximal_loss}. We did not need this trick since our CNNs reached 0 training error in all experiments. Using this assumption, we show that the loss over the entire dataset can be bounded by the Bayes optimal classifier and the result of our discrete optimization problem.

\begin{theorem}
Given $n$ i.i.d. samples drawn from $p_\mathcal{Z}$ as $\{\mathbf{x}_i,y_i\}_{i\in[n]}$, and $m$ chosen points $\{ s(i) \in [N]\}_{\i \in [m]}$. If loss function $l(\cdot,y,\mathbf{w})$ is $\lambda^l$-Lipschitz continuous for all $y, \mathbf{w}$, regression function is $\lambda^\eta$-Lipschitz, $\{ s(i) \in [N]\}_{\i \in [m]}$ is $\delta$ cover of $\{\mathbf{x}_i,y_i\}_{i\in[n]}$, $l(\mathbf{x}_{s(j)},y_{s(j)},A_\mathbf{S})=0\quad \forall j \in [m]$; with probability at least $1-\gamma$,
\begin{small}
\[
\frac{1}{n}\sum_i l(\mathbf{x}_i,y_i,A_\mathbf{s}) \leq \mathcal{L}_{[n]} (h^\star) +\delta(\lambda^l + 2 \lambda^{\eta}) + 
\sqrt{\frac{\log(1/1-\gamma)}{2n}}
\]
\end{small}
where $\mathcal{L}_{[n]} (h^\star)$ is the loss of the Bayes-optimal classifier.
\label{mainthm2}
\end{theorem}

It can easily be shown that; in this setting, $\lim_{n \rightarrow \infty} \frac{1}{n}\sum_i l(\mathbf{x}_i,y_i,A_\mathbf{s}) =   \mathcal{L}_{[n]} (h^\star) +\delta(\lambda^l + 2 \lambda^{\eta})$. Clearly, $\delta$ decreases when $m$ increases; however, the rate is critical. To show that our algorithm sounds, we need to show that $\delta$ can be made arbitrarily small with finite $m$ in the limiting behavior of number of unlabelled data points (i.e. $n \rightarrow \infty$). Since our data points are coming from a compact space, there exist a finite sub-cover to any union of open sets. Hence, the finite query property is straightforward result of compactness. We give the following corollary without proof;

\begin{cor}
Given $n$ i.i.d. samples drawn from $p_\mathcal{Z}$ as $\{\mathbf{x}_i,y_i\}_{i\in[n]}$, and a desired error rate $\rho$. If loss function $l(\cdot,y,\mathbf{w})$ is $\lambda^l$-Lipschitz continuous for all $y, \mathbf{w}$, regression function is $\lambda^\eta$-Lipschitz, there exist a finite subset $\mathbf{s}$ with cardinality m such that any CNN achieving $0$ error over $\{\mathbf{x}_{s(j)},y_{s(j)}\}_{j \in [m]}$ achieve the following with probability $1$.
\[
\lim_{n \rightarrow \infty} \frac{1}{n}\sum_i l(\mathbf{x}_i,y_i) \leq \mathcal{L}_{[n]} (h^\star) +\rho
\]
where $\mathcal{L}_{[n]} (h^\star)$ is the loss of the Bayes-optimal classifier.
\label{maincor}
\end{cor}

In summary, we show that CNNs have Lipschitz continuous loss functions, making them generalizes to unseen images. In addition, when the underlying data distribution has Lipschitz continuous regression functions, we further show, under reasonable assumptions, a small subset of dataset is enough to be labelled as long as it covers the space efficiently. Consider the fact that the difference between the empirical loss over unseen images and the optimal loss is bounded by $\delta(\lambda^l + 2 \lambda^{\eta})$. Moreover, our algorithm is based on direct minimization of $\delta$. Hence, our proposed method directly minimizes this loss and theoretical analysis validates our space-covering heuristic.
