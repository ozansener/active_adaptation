\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{wrapfig}
\usepackage{graphicx} % more modern
\usepackage{caption}
\usepackage{subcaption}

% AMS stuff
\usepackage{amsmath, amsthm, amssymb}
\usepackage{dsfont}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{booktabs}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{xspace}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}

\usepackage{color}
\newcommand{\todo}[1]{{\bf \color{red}[todo: #1]}}

\title{Geometric Approach to Active Learning for CNNs}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labelled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (\ie active learning)

In this paper, we first show that existing active learning heuristics are not effective for CNNs even in an oracle setting. Our counterintuitive empirical results make us question these heuristics and inspire us to come up with a simple but effective method, choosing a set of images to label such that they cover the set of unlabelled images as close as possible. We further present a theoretical justification for this geometric heuristic by giving a bound over the generalization error of CNNs. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.
\end{abstract}

\section{Introduction}
Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of active research in computer vision and pattern recognition. like image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their millions of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this is the behavior you want from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations bring a critical question: \emph{``what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget.''} Active learning is one of the common paradigms to address this question.

The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy \cite{dasgupta2004analysis}, there exist many heuristics~\cite{settles2010active} which have been proven to be effective in practice. To the best of our knowledge, almost none of these methods are utilized for deep learning since they typically are not effective for CNNs. The prevalent belief for this behavior is CNNs' tendency to make very confident mistakes. It is empirically observed that when CNNs make mistakes, they can assign arbitrary confidence values to their decisions. In other words, it is typically not possible to deduce that a CNN is uncertain by solely looking at its outputs. Although we agree with this observation, our empirical analysis suggests that this is not the main reason behind the ineffectiveness of active learning for CNNs. 

Following our empirical study, we decide not to adopt an uncertainty based method and approach to the problem of active learning from a geometric perspective. We hypothesize that given a large unlabeled dataset, the desired property of the set of labeled points is to cover the set of unlabelled points as closely as possible \emph{(See Figure~\ref{fig:tsne} for a visual explanation)}. In other words, we find a set of points to label such that when they are labeled, every remaining unlabelled point in the dataset will have a close labeled neighbor. We formulate this space-covering property as an optimization problem and present an efficient solution.

We carry out an in-depth analysis of our algorithm both theoretically and empirically. We study the generalization error of CNNs in a realistic setting and present a bound on the difference between the population risk and the empirical risk. We further consider the active learning case and present a bound over the risk of the unlabelled data points in terms of the distance between unlabelled points and their labeled nearest neighbors. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin. 

%In summary, contributions of this work are; i) first successful application of active learning on CNNs which is effective both with semi-supervision and without it, ii) an empirical study on limitations of uncertainty based active learning in CNNs, iii) theoreti 

\section{Related Work}
We discuss the related work in the following categories separately. Briefly, our work is different from existing approaches since $i)$ it specifically targets CNNs, $ii)$ we consider both fully supervised and weakly supervised cases, and $iii)$ we theoretically analyze our algorithm.

\noindent\textbf{Active Learning}
Active learning has been widely studied and most of the early work can be found in the classical survey \cite{settles2010active}. It discusses most query strategies such as information theoretical methods \cite{mackay1992information}, ensemble approaches \cite{mccallumzy1998employing, freund1997selective} and uncertainty based methods \cite{tong2001support,lewissequential,joshi2009multi,li2013adaptive}. %We will try to review the literature coming after \cite{settles2010active}. 

Bayesian active learning methods typically use a non-parametric model like Gaussian process to estimate the expected improvement by each query \cite{kapoor2007active} or expected error after a set of queries \cite{roy2001toward}. These approaches are typically not applicable to deep learning scenarios since they do not scale to large-scale datasets. Ensemble methods are also not applicable to deep learning due to the large parameter space of neural networks. Such ensemble methods require an intractable number of networks to be trained to be effective.

One important class is that of uncertainty based methods, which try to find hard examples using heuristics like highest entropy \cite{joshi2009multi}, and geometric distance to decision boundaries \cite{tong2001support,brinker2003incorporating}. We present an empirical result in Section~\ref{sec:whatif} which motivated us to move away from such techniques. We empirically demonstrate that even in the oracle case, such algorithms are typically not effective for CNNs.

There are recent optimization based approaches which can trade-off uncertainty and diversity to obtain a diverse set of hard examples. Elhamifar~et al.  \cite{elhamifar2013convex} design a discrete optimization problem for this purpose and use its convex surrogate. However, the algorithm uses $n^2$ variables where $n$ is the number of data points. Hence, it does not scale to the deep learning case. There are also many discrete optimization based active learning algorithms designed for the specific class of machine learning algorithms like k-nearest neighbors and naive Bayes \cite{wei2015submodularity}. Even in the algorithm agnostic case, one can design a set-cover algorithm to cover the hypothesis space using sub-modularity \cite{guillory2010interactive, golovin2011adaptive}. Our algorithm can be considered to be in this class; however, we do not use any uncertainty information. Our algorithm is also the first one which applies to the CNNs.

Recently, a discrete optimization based algorithm \cite{BerlindU15} which is similar to ours has been presented for k-nearest neighbors type of algorithms in domain shift setting. Although our theoretical analysis borrows many techniques from \cite{BerlindU15}, their results are only valid for k-NN and are not applicable to CNNs.

To the best-of-our-knowledge, the only active learning algorithm designed for CNNs is presented in \cite{wang2016cost}. It is a heuristic based transductive algorithm which directly assigns labels to the data points with high-confidence and queries labels for the ones with low confidence. We discuss its limitations in Section~\ref{sec:exp}.

\noindent\textbf{Unsupervised Subset Selection}
The closest literature to our work is the problem of unsupervised subset selection. This problem considers a fully labeled dataset and tries to choose a subset of it such that the model trained on the selected subset will perform as closely as possible to the model trained on the entire dataset. For specific learning algorithms, there are methods like core-sets for SVM \cite{tsang2005core} and core-sets for k-Means and k-Medians \cite{har2005smaller}. %However, there is no such method for CNNs.

The most similar to our algorithm is the unsupervised subset selection algorithm described in \cite{wei2013using}. It uses a facility location problem to find a diverse cover for the dataset. Our algorithmic difference is using a slightly different formulation of facility location problem. Instead of the min-sum, we use the minimax \cite{facility} form of the facility location. More importantly, we apply this algorithm for the first time to the problem of active learning and provide a strong theoretical guarantees.
 
\noindent\textbf{Weakly-Supervised Deep Learning}
Our paper is also related to semi-supervised deep learning since we experiment the active learning both in the fully-supervised and weakly-supervised scheme. 
One of the early weakly-supervised convolutional neural network algorithms was Ladder networks \cite{ladder}. Recently, we have seen adversarial methods which can learn a data distribution as a result of a two-player non-cooperative game \cite{salimans2016improved,gan_original,dcgan}. These methods are further extended to feature learning \cite{ali, bigan}. We use Ladder networks in our experiments since adversarial architectures are notoriously hard to train. Our algorithm is agnostic to the weakly-supervised learning algorithm choice and can easily use any weakly-supervised or fully-supervised model.

\section{Problem Definition}
In this section, we formally define the problem of active learning and setup the notation for the rest of the paper. We are interested in a $C$ class classification problem defined over a compact space $\mathcal{X}$ and a label space  $\mathcal{Y}=\{1,\ldots,C\}$. We also consider a loss function $l(\cdot,\cdot;\mathbf{w}):\mathcal{X}\times \mathcal{Y} \rightarrow \mathcal{R}$ parametrized over the hypothesis class ($\mathbf{w}$), e.g.\ parameters of the deep learning algorithm. We further assume class-specific regression functions $\eta_c(\mathbf{x})=p(y=c|\mathbf{x})$ to be \mbox{$\lambda^\eta$-Lipschitz} continuous for all $c$.

We consider a large collection of data points which are sampled $i.i.d.$ over the space  $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$ as \mbox{$\{\mathbf{x}_i,y_i\}_{i \in [n]} \sim p_\mathcal{Z}$} where $[n]=\{1,\ldots,n\}$. We further consider an initial pool of data-points chosen uniformly at random among them as \mbox{$\mathbf{s}^0=\{s^0(j) \in [n]\}_{j \in [m]}$}.

An active learning algorithm has only access to $\{\mathbf{x}_i\}_{i \in [n]}$ and $\{y_{s(j)}\}_{j \in [m] }$. In other words, it can only see the labels of the points in the initial sub-sampled pool. It is also given a budget $b$ of queries to ask to an oracle and a learning algorithm $A_{\mathbf{s}}$ which outputs a set of parameters $\mathbf{w}$ given a labelled set $\mathbf{s}$. The active learning with a pool problem can simply be defined as
\begin{equation}
\min_{\mathbf{s}^1 : |\mathbf{s}^1| \leq b} E_{\mathbf{x},y \sim p_\mathcal{Z}} [l(\mathbf{x},y; A_{\mathbf{s}^0 \cup \mathbf{s}^1})]
\end{equation}
In other words, an active learning algorithm can choose $b$ extra points and get them labelled by an oracle to minimize the future expected loss.

There are a few differences between our formulation and the classical definition of active learning. Classical methods consider the case in which the budget is 1 ($b=1$) but a single point has negligible effect in a deep learning regime hence we consider the batch case. It is also very common to consider multiple rounds of this game. %as in each round $k$,% solving the following
%\begin{equation}
%\min_{\mathbf{s}^{k+1} : |\mathbf{s}^{k+1}| \leq b} E_{\mathbf{x},y \sim p_\mathcal{Z}} [l(\mathbf{x},y;A_{\mathbf{s}^{0} \cup \ldots  \mathbf{s}^{k+1} \cup \ldots  \mathbf{s}%^{k+hor}})]
%\end{equation}
%Where $hor$ is the number of active learning iterations expected to be performed. 
%Although it is more realistic, analysis is typically trickier in such a setting. Hence, 
We also follow the multiple round formulation with a myopic approach by solving the single round of labelling as;
\begin{equation}
\min_{\mathbf{s}^{k+1} : |\mathbf{s}^{k+1}| \leq b} E_{\mathbf{x},y \sim p_\mathcal{Z}} [l(\mathbf{x},y;A_{\mathbf{s}^{0} \cup \ldots  \mathbf{s}^{k+1}})]
\end{equation}
In other words, our method solves the multi-stage problem in a myopic way without utilizing look-ahead. In the rest of the paper we discuss the fist iteration where $k=0$ for brevity although we apply it over multiple rounds. 

At each iteration, an active learning algorithm has two stages: 1. identifying a set of data-points and presenting them to an oracle to be labelled, and 2. training a classifier using these new as well as the previously labeled data-points. The second stage (training the classifier) can be done in a fully or weakly-supervised manner. Fully-supervised is the case where training the classifier is done using only the labeled data-points. Weakly-supervised is the case where training also utilizes the points which are NOT labelled yet. Although the existing literature only focus on the active learning for fully-supervised models, we considered both cases and experimented on both. 


\section{Active Learning as a Set Cover}
When there is no direct measure of uncertainty over the hypothesis class, the active learning problem is typically considered as refining decision boundaries by querying hard examples. Hence, using uncertainty is an empirically proven heuristic. However, this heuristic has very limited success in CNNs. This is widely attributed to the fact that the CNNs typically make very confident mistakes and the confidence values computed via soft-max outputs do not correspond to the true confidence of the model. Here, we focus on the following more fundamental question; \emph{would classical query methods work for CNNs if CNNs had an accurate uncertainty estimate?}. Although the common sense answer is affirmative, our empirical analysis shows that this is typically not the case. We describe this experiment in detail in Section~\ref{sec:whatif}.

Empirical observation on the ineffectiveness of uncertainty based approaches inspired us to design a fundamentally different method for active learning for CNNs. We propose to not use uncertainty information, and approach to the problem from a purely geometric perspective. We design an algorithm based on the heuristic of covering the set of unlabeled data points as closely as possible. We explain this algorithm in detail in Section~\ref{sec:alg} and further analyze it empirically in Section~\ref{sec:exp} and theoretically in Section~\ref{sec:analysis}.

\subsection{Ineffectiveness of Uncertainty based Methods}
\label{sec:whatif}
It is very common to attribute the ineffectiveness of uncertainty methods in CNNs to the fact that uncertainty estimates based on soft-max outputs are typically not accurate. Hence, it is very common to hypothesize the following: Deep learning algorithms lead to an inaccurate estimate of uncertainty hence the uncertainty based active learning methods fail with CNNs. Although this hypothesis is very intuitive considering the many confident mistakes CNNs typically make, it is not enough to answer more fundamental question: If CNNs produced accurate estimates of uncertainty, would uncertainty based active learning methods work for CNNs?

We can answer this question by simply replacing the uncertainty estimate in active learning with oracle ground truth loss. In other words, we replace the uncertainty with $l(\mathbf{x}_i,y_i,A_{\mathbf{s}^0})$ for all unlabelled examples $\mathbf{x}_i$. Since this is the oracle for the estimation of the uncertainty, in practice the uncertainty based methods are expected to be upper bounded by this oracle. We sample the queries from the normalized form of this function by setting the probability of choosing the $i^{th}$ point to be queried as $p_i=\frac{l(\mathbf{x}_i,y_i,A_{\mathbf{s}^0})}{\sum_j l(\mathbf{x}_j,y_j,A_{\mathbf{s}^0})}$. We use two loss functions, classification accuracy \mbox{$\mathds{1}[y_i = \argmax_c CNN_c(\mathbf{x}_i;A_{\mathbf{s}^0})]$} and cross entropy \mbox{$ - CNN_{y_i}(\mathbf{x}_i;A_{\mathbf{s}^0}) -\sum_{c \in [C] \setminus y_i} \log(1-  CNN_{c}(\mathbf{x}_i;A_{\mathbf{s}^0}))$} where $\mathds{1}[\cdot]$ is the indicator function and $CNN_c(\cdot,\mathbf{w})$ is the activation of $c^{th}$ softmax output given network weights $\mathbf{w}$. As the oracle, we use the maximum accuracy obtained by querying based on either of these loss functions. We perform this experiment for fully supervised and weakly supervised case and plot the results in Figure~\ref{fig:neg}. \emph{(See Section~\ref{sec:imp} for implementation details)}

Results in Figure~\ref{fig:neg} suggest that even in the oracle case, uncertainty based methods are not effective for CNNs when compared with random sampling. {\parfillskip0pt\par}

\begin{wrapfigure}{t}{0.5\textwidth}
\vspace{-10mm}
  \begin{center}
    \begin{subfigure}[b]{0.23\textwidth}
		\includegraphics[width=\columnwidth]{fig1_a_1.pdf}
		\vspace{-5mm}
		\caption{Uncertainty Oracle}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
		\includegraphics[width=\columnwidth]{fig1_b_1.pdf}
		\vspace{-5mm}
		\caption{Our Method}
    \end{subfigure}
\end{center}
\vspace{-3mm}
 \caption{tSNE embeddings of images for active learning using uncertainty oracle and our algorithm. We further choose a step in active learning with 5000 images in initial pool and algorithms need to choose 1000 more samples and color the initial pool of images with blue, the actively chosen images with green, and remaining images with red. The uncertainty oracle creates a bias leaving a significant part of the dataset uncovered. }
\label{fig:tsne}
\vspace{-10mm}
\end{wrapfigure}



\noindent We even observe that it causes the accuracy to drop in the weakly-supervised case. Hence, the aforementioned hypothesis is not entirely correct at least for the batch setting. Hence, we can further conclude, Inaccurate estimate of uncertainty does not explain the failure of uncertainty based active learning methods in CNNs in batch setting.

We believe this counterintuitive result is mostly due to the fact that we are sampling/labelling images in batches instead of querying one by one. The batch sampling of queried samples creates strong correlation among the chosen data points. On the other hand, querying images one by one is not desired since a single point has no significant effect in deep learning due to the SGD. In order to fix this issue, we perform the same experiment with 20\% exploration as sampling points from the oracle density 80\% of the time and sampling uniformly at random remaining 20\% of the time. We also plot oracle with exploration in the same figure. Although it helps, the oracle still does not perform better than random baseline.

%We further discuss the failure of the uncertainty based oracle and try to motivate our method. 
In order to visualize the bias, consider an embedding of images computed using the tSNE\cite{tsne} algorithm based on features learned after incorporating the entire dataset in learning. We plot the images in the initial pool ($\mathbf{s}^0$), chosen images for labelling ($\mathbf{s}^1$), and remaining images ($[n] \setminus (\mathbf{s}^0 \cup \mathbf{s}^1$)) with separate colors in Figure~\ref{fig:tsne}-a. As shown in Figure~\ref{fig:tsne}-a, the oracle algorithm samples the data points with a bias hence the resulting samples do not cover the space efficiently. A successful set of queries not only need to be hard-negatives but also need to cover the space efficiently. Hence, we believe covering the space effectively is very important for CNNs and we design an algorithm purely based on space covering in Section~\ref{sec:alg}. We also show the tSNE plot for our algorithm in Figure~\ref{fig:tsne}-b.

\begin{figure}[t]
\vspace{-3mm}
\includegraphics[width=\columnwidth]{fig1_2_h.pdf}
\vspace{-5mm}
\caption{\todo{add 0.8 exploration} Comparison of random sampling and active learning with oracle loss information. This figure suggests that even with oracle loss estimates, uncertainty based active learning algorithms are not effective for CNNs. Specifically for the case of active learning for weakly-supervised models, random sampling even outperforms oracle loss estimates. This is not surprising since weakly-supervised algorithms relies heavily on feature learning and having a bias over the dataset would cause inaccurate feature learning.}
\label{fig:neg}
\end{figure}

\subsection{The Algorithm}
\label{sec:alg}
  \begin{wrapfigure}{R}{0.5\textwidth}
    \begin{minipage}{0.5\textwidth}
    \vspace{-8mm}
\begin{algorithm}[H]
   \caption{k-Center-Greedy}
      \label{alg:greedy}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $\mathbf{x}_i$, existing pool $\mathbf{s}^0$ and a budget $b$
    \STATE Initialize $\mathbf{s}=\mathbf{s}^0$
   \REPEAT
   \STATE $u=\arg\max_{i \in [n] \setminus \mathbf{s}} \min_{j \in \mathbf{s}} \Delta(\mathbf{x}_i, \mathbf{x}_j)$
   \STATE $\mathbf{s} = \mathbf{s} \cup \{u\}$
   \UNTIL {$|\mathbf{s}|<b+|\mathbf{s}^0|$}
   \STATE {\bfseries return} $\mathbf{s} \setminus \mathbf{s}^0$
\end{algorithmic}
\end{algorithm}
\vspace{-8mm}
    \end{minipage}
  \end{wrapfigure}

We hypothesize that a good way to choose points to be labelled is trying to cover the unlabelled data points as close as possible with labeled points. For example, consider a set of balls with radius $\delta$ centered at labelled points covering the entire unlabelled dataset. Intuitively, smaller $\delta$ should indicate a better performance. Hence, we try to choose a subset of points which can minimize $\delta$ as an active learning strategy. 
  
    \begin{wrapfigure}{R}{0.5\textwidth}
    \begin{minipage}{0.5\textwidth}
    \vspace{-8mm}
\begin{algorithm}[H]
   \caption{Robust k-Center}
   \label{alg:bin}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $\mathbf{x}_i$, existing pool $\mathbf{s}^0$, budget $b$ and outlier bound $\Xi$
   \STATE {\bfseries Initialize} $\mathbf{s}_g =$ k-Center-Greedy($\mathbf{x}_i, \mathbf{s}^0, b$)
   \STATE $\delta_{2-OPT} = \max_j \min_{i \in \mathbf{s}_g} \Delta(\mathbf{x}_i,\mathbf{x}_j)$ 
   \STATE $lb=\frac{\delta_{2-OPT}}{2}$, $ub=\delta_{2-OPT}$
   \REPEAT
   \IF {$Feasible(b, \mathbf{s}^0,\frac{lb+ub}{2},\Xi)$}
   \STATE $ub=\max_{i,j \mid  \Delta(\mathbf{x}_i,\mathbf{x}_j) \leq \frac{lb+ub}{2}}  \Delta(\mathbf{x}_i,\mathbf{x}_j) $
   \ELSE
   \STATE $lb=\min_{i,j \mid   \Delta(\mathbf{x}_i,\mathbf{x}_j) \geq \frac{lb+ub}{2}}  \Delta(\mathbf{x}_i,\mathbf{x}_j) $
    \ENDIF
   \UNTIL{$ub = lb$}
      \STATE {\bfseries return} $\{i\ s.t.\ u_i=1\}$
\end{algorithmic}
\end{algorithm}
\vspace{-5mm}
    \end{minipage}
  \end{wrapfigure}

Our algorithm is simply based on the \emph{k-Center} problem (minimax facility location problem \cite{facility}) which can intuitively be defined as follows; choose $k$ center points such that the largest distance between a data point and its nearest center is minimized. Formally, we are trying to solve:
\begin{equation}
\min_{\mathbf{s}^1} \max_i \min_{j \in \mathbf{s}^1 \cup \mathbf{s}^0} \Delta(\mathbf{x}_i,\mathbf{x}_j)
\end{equation}



Unfortunately this problem is NP-Hard \cite{cook}. However, it is possible to obtain a $2-OPT$ solution efficiently using a greedy approach shown in  Algorithm~\ref{alg:greedy}. If $OPT=\min_{\mathbf{s}^1} \max_i \min_{j \in \mathbf{s}^1 \cup \mathbf{s}^0} \Delta(\mathbf{x}_i,\mathbf{x}_j)$, the greedy algorithm shown in Algorithm~\ref{alg:greedy} is proven to have a solution ($\mathbf{s}^1$) such that; $ \max_i \min_{j \in \mathbf{s}^1 \cup \mathbf{s}^0} \Delta(\mathbf{x}_i,\mathbf{x}_j) \leq 2 \times OPT$.

Although the greedy algorithm gives a good initialization, in practice we can improve the $2-OPT$ solution by iteratively querying upper bounds on the optimal value. In other words, we can design an algorithm which decides if $OPT \leq \delta$. In order to do so, we define a mixed integer program (MIP) parametrized by $\delta$ such that its feasibility indicates $\min_{\mathbf{s}^1} \max_i \min_{j \in \mathbf{s}^1 \cup \mathbf{s}^0} \Delta(\mathbf{x}_i,\mathbf{x}_j) \leq \delta$. A straight-forward algorithm is using this MIP as a sub-routine and performing a binary search between the result of the greedy algorithm and its half since the optimal solution is guaranteed to be included in that range. While constructing this MIP, we also try to handle one of the weaknesses of k-Center algorithm, namely robustness. To make the k-Center problem robust, we assume an upper limit on the number of outliers $\Xi$ such that our algorithm can choose not to cover at most $\Xi$ unsupervised data points. This mixed integer program can be written as:

\begin{equation}
\begin{aligned}
Feasible(b,\mathbf{s}^0,\delta, \Xi):  &\sum_j  u_j, = |\mathbf{s}^0|+ b,  \quad &&  \sum_{i,j} \xi_{i,j} \leq \Xi \\
&\sum_j \omega_{i,j} = 1\quad \forall  i, \quad && \omega_{i,j} \leq u_j \quad \forall  i,j \\
   & u_j =1 \quad \forall j\in \mathbf{s}^0, \quad && \omega_{i,j} = \xi_{i,j} \quad  \forall i,j \mid   \Delta(\mathbf{x}_i,\mathbf{x}_j)  > \delta\\
\end{aligned}
\label{mipfeasible}
\end{equation}

In this formulation, $u_i$ is 1 if $i^{th}$ data point is chosen as center, $\omega_{i,j}$ is $1$ if $i^{th}$ point is covered by $j^{th}$ point and $\xi_{i,j}$ is 1 if $i^{th}$ point is an outlier and included in $j^{th}$ point without the $\delta$ constraint, and $0$ otherwise. And, variables are binary as $u_i, \omega_{i,j}, \xi_{i,j} \in \{0,1\}$. We further visualize these variables in a diagram in Figure~\ref{mip}, and give the complete method in detail in Algorithm~\ref{alg:bin}. 


\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-5mm}
  \begin{center}
\includegraphics[width=0.38\textwidth]{mip.pdf}
\end{center}
    \caption{Visualizations of the variables in the mixed integer program. In this solution, the $4^{th}$ node is chosen as a center and node $0,1,3$ ended up in a $\delta$ ball around the $4^{th}$ node. The solution also marked the $2^{nd}$ node as an outlier by not including it in any $\delta$ ball.}
\label{mip}
\vspace{-12mm}
\end{wrapfigure}



\subsection{Implementation Details}
\label{sec:imp}
One of the most important design choices is the distance function $\Delta(\cdot,\cdot)$. We use the $l_2$ distance between activations of the final fully-connected layer as a distance function. For semi-supervised learning, we used Ladder networks \cite{ladder} and for all experiments we used VGG-16 \cite{vgg} as the CNN architecture. We optimized all models using RMSProp with a learning rate of $1\mathrm{e}{-3}$ using Tensorflow \cite{tensorflow}. We train CNNs from scratch after each iteration of the active learning step.

While implementing our algorithm, we used Gurobi \cite{gurobi} framework for checking feasibility of the MIP defined in (\ref{mipfeasible}). As an upper bound on number of outliers, we used $\Xi=1\mathrm{e}{-4} \times n$ where $n$ is the number of unlabelled data points.

%Upon acceptance, we are planning to release the trained models and the source code.
%\clearpage
\section{Analysis of the Algorithm}
\label{sec:analysis}
\input{analysis}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{ws_fig2.pdf}
    \end{subfigure}
    \vspace{-5mm}
    \caption{Results on Active Learning for Weakly-Supervised Model (error bars are 3-std-dev)}\label{fig:ressemi}
        \vspace{-3mm}
    \label{fig:resns}
%\end{figure*}
%\begin{figure*}[ht]
%    \centering
   \vspace{5mm}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{fs_fig2.pdf}
    \end{subfigure}
        \vspace{-5mm}
    \caption{Results on Active Learning for Fully-Supervised Model (error bars are 3-std-dev)}\label{fig:resnosemi}
        \vspace{-5mm}
    \label{fig:ress}
\end{figure}

\section{Experimental Results}
\label{sec:exp}

To evaluate our approach, we tested our algorithm on the problem of classification using three different datasets. We performed experiments on CIFAR\cite{cifar} and Caltech-256\cite{caltech256} datasets for image classification and on SVHN\cite{svhn} dataset for digit classification. Moreover, CIFAR\cite{cifar} dataset has two tasks; one coarse-grained and one fine-grained. There are 100 fine-grained categories and 10 coarse-grained categories defined as strict supersets of some of these fine-grained categories. We performed experiments on both tasks.

We also conducted experiments on active learning for fully-supervised models as well as active learning for weakly-supervised models. In our experiments, we start with 1.5\% of the images sampled uniformly at random from the dataset as an initial pool. The weakly-supervised model has access to labeled examples with their labels as well as unlabelled examples without their labels. The fully-supervised model only has access to the labeled data points. We use ladder networks\cite{ladder} as a weakly-supervised model and the VGG16\cite{vgg} as a fully supervised model. We run all experiments with five random initializations of the initial pool of labeled points and use the average classification accuracy as a metric and plot the accuracy vs. the number of labeled points. We also plot error bars as three standard deviations. We run the query algorithm iteratively; in other words, we solve the discrete optimization problem $\min_{\mathbf{s}^{k+1} : |\mathbf{s}^{k+1}| \leq b} E_{\mathbf{x},y \sim p_\mathcal{Z}} [l(\mathbf{x},y; A_{\mathbf{s}^{0} \cup \ldots, \mathbf{s}^{k+1}})]$ for each point on the accuracy vs number of labelled examples graph. We present the results in Figure~\ref{fig:ressemi}\&\ref{fig:resnosemi}.

%At each step, we learned a network from scratch using $\mathbf{s}^{k} \cup \mathbf{s}^{k+1}$. 

We compare our algorithm with uniformly at random sampling as well as the uncertainty oracle explained in Section~\ref{sec:whatif}. We also compared our algorithm with CEAL \cite{wang2016cost} which is to the best-of-our-knowledge, only active learning algorithm presented for CNNs. Since it is a weakly-supervised approach utilizing unlabeled data points, we only include it in the weakly-supervised analysis.

 \begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=0.48\textwidth]{mip_100_2.pdf}
\caption{We compare our method with its greedy (k-CenterGreedy) version. Our algorithm provides both robustness and optimality with an additional computation cost and causes a small but not negligible accuracy improvement. }
\label{fig:twoopt}
\end{wrapfigure}

Figure~\ref{fig:resns} and \ref{fig:ress} suggests that our algorithm outperforms all other baselines in all experiments; for the case of weakly-supervised models, with a large margin. We believe the effectiveness of our approach in the weakly-supervised case is due to the better feature learning. Weakly-supervised models provide better feature spaces resulting in accurate geometries. Since our method is geometric, it performs significantly better with better feature spaces. We also observed that our algorithm is less effective in CIFAR-100 and Caltech-256 when compared with CIFAR-10 and SVHN. This can easily be explained using our theoretical analysis. Our generalization bound scales linearly with the number of classes hence it is better to have fewer classes.

\noindent\textbf{Optimality of the k-Center Solution:} Our proposed method is using the greedy 2-OPT solution for the k-Center problem as an initialization and checks the feasibility of a mixed integer programming (MIP) program. Internally, we use LP-relaxation of the defined MIP and use branch-and-bound on the solution of the relaxed LP. MIP does not enjoy a polynomial time solution; hence, the utility obtained by solving this expensive MIP should be investigated. We compare the average run-time of MIP\footnote{On Intel Core i7-5930K@3.50GHz and 64GB memory} with the run-time of 2-OPT solution in Table~\ref{tab:runtime}. We also compare the accuracy obtained with optimal k-Center solution and the 2-OPT solution in Figure~\ref{fig:twoopt}


\begin{table}[ht]
\centering
\vspace{-3mm}
\caption{Decomposition of average run-time of our discrete optimization algorithm for $b=5000$ and $|\mathbf{s}^0|=10000$ in seconds.}
\begin{tabular}{ccccc} \toprule
 Distance& Greedy & MIP & MIP &  \\
Matrix &(2-OPT) & (per iteration) & (total) & Total \\ \midrule
104.2  & 2   & 7.5  &  244.03  & 360.23  \\ \bottomrule
\end{tabular}
\label{tab:runtime}
\vspace{-2mm}
\end{table}

As shown in the Table~\ref{tab:runtime}; although the run-time of MIP is not polynomial in worst-case, in practice it converges in a tractable amount of time for a dataset of 50k images. Hence, our algorithm can easily be applied in practice. Figure~\ref{fig:twoopt} suggests a small but significant drop on the accuracy when 2-OPT solution is used. Hence, we conclude that unless the scale of the dataset restricts, using our proposed optimal solver is desired. Even with the accuracy drop, our active learning strategy using 2-OPT solution still outperforms the other baselines. Hence, we can conclude that our algorithm can scale to any dataset size with small accuracy drop even if solving MIP is not feasible.

\section{Conclusion}
We described an active learning algorithm for CNNs. Our empirical analysis showed that classical uncertainty based methods have limited applicability to the CNNs. We design a simple but effective active learning algorithm for CNNs using geometric intuitions. We further validated our algorithm using both theoretical analysis and an empirical study. Empirical results on CIFAR\cite{cifar} dataset showed state-of-the-art performance with a large margin specifically for the semi-supervised case.


%{\small
\bibliography{active_adversarial} 
\bibliographystyle{abbrv}
%}

\appendix
\input{proofs}
\section{Unsupervised subset selection} 

Since our algorithm does not use any uncertainty information, we can also use it for unsupervised subset selection. In other words, given an informative distance function, we can use our algorithm to choose subset of the training set. In unsupervised subset selection problem, the constraint is using subset of the dataset due to the resource constraints. Given a dataset size constraint, the problem is choosing a subset of examples such that the trained model will perform as closely as possible to the model trained on entire dataset. One big difference to active learning is lack of iteration. Given a budget, subset is selected and not refined/improved.


We perform this experiment with $l_2$ distance of features learned with no supervision as $\Delta(\cdot,\cdot)$. We use \cite{improved_gan} for unsupervised feature learning using their shared source code. We plot the accuracy vs dataset size using both our algorithm and uniformly random selection in Figure~\ref{fig:scat} and conclude that our algorithm is effective for the problem of unsupervised subset selection.  

\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{u_100_2.pdf}
\caption{Experiment on unsupervised subset selection. We use our algorithm with ImprovedGAN\cite{improved_gan} features in unsupervised subset selection setup. Results suggest that our algorithm is more effective than uniform random sampling .}
\label{fig:scat}
\end{wrapfigure}

\end{document}
