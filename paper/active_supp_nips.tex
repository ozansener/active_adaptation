\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx} % more modern
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}

% AMS stuff
\usepackage{amsmath, amsthm, amssymb}
\usepackage{dsfont}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{booktabs}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{xspace}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}


\title{Active Learning for Convolutional Neural Networks\\ Supplementary Materials}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
In this supplementary materials, we present the proofs which are skipped in the main text due to the space constraint.
\end{abstract}

\input{proofs}
\section{Unsupervised subset selection} 

Since our algorithm does not use any uncertainty information, we can also use it for unsupervised subset selection. In other words, given an informative distance function, we can use our algorithm to choose subset of the training set. In unsupervised subset selection problem, the constraint is using subset of the dataset due to the resource constraints. Given a dataset size constraint, the problem is choosing a subset of examples such that the trained model will perform as closely as possible to the model trained on entire dataset. One big difference to active learning is lack of iteration. Given a budget, subset is selected and not refined/improved.


We perform this experiment with $l_2$ distance of features learned with no supervision as $\Delta(\cdot,\cdot)$. We use \cite{improved_gan} for unsupervised feature learning using their shared source code. We plot the accuracy vs dataset size using both our algorithm and uniformly random selection in Figure~\ref{fig:scat} and conclude that our algorithm is effective for the problem of unsupervised subset selection.  

\begin{figure}[t]
\includegraphics[width=\textwidth]{u_100_2.pdf}
\caption{Experiment on unsupervised subset selection. We use our algorithm with ImprovedGAN\cite{improved_gan} features in unsupervised subset selection setup. Results suggest that our algorithm is more effective than uniform random sampling .}
\label{fig:scat}
\end{figure}

%{\small
\bibliography{active_adversarial} 
\bibliographystyle{abbrv}
%}




\end{document}
